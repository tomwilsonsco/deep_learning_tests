{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "linear_regression_grad_descent_ example.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "uZHJUR7MaW6X"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 118,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y11o8ty6YRLO"
      },
      "source": [
        "Notebook that aims to build my intuition on gradient descent, loss functions and learning rates<br>\n",
        "This uses a simple linear regression to show how model parameters are updated by gradient descent using RMSE as a loss function<br><br>\n",
        "Data taken from Deep Learning with PyTorch, but here only using Numpy <br>\n",
        "Some help taken from https://ml-cheatsheet.readthedocs.io/en/latest/linear_regression.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rOkzJZAHQOyc"
      },
      "source": [
        "y = np.array([0.5,  14.0, 15.0, 28.0, 11.0,  8.0,  3.0, -4.0,  6.0, 13.0, 21.0])\n",
        "x = np.array([35.7, 55.9, 58.2, 81.9, 56.3, 48.9, 33.9, 21.8, 48.4, 60.4, 68.4])"
      ],
      "execution_count": 119,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6rLsQbi-oLBO"
      },
      "source": [
        "x is an array of temperatures in fahrenheit, y equivalent in celcius<br>\n",
        "Can gradient descent be used to estimate weight and bias parameters for a linear formula to convert fahrenheit to celcius?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZT1mBMoOQdEG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "outputId": "9ffc343f-aff6-44c5-86dc-3e4ceac6ae6e"
      },
      "source": [
        "plt.scatter(x, y)"
      ],
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.collections.PathCollection at 0x7f9f1c3192e8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 120
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IQp4eBuuQjxl"
      },
      "source": [
        "def lin_pred(x, w, b):\n",
        "  \"\"\"linear function\"\"\"\n",
        "  return x * w + b"
      ],
      "execution_count": 121,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4oGWbNINTfja"
      },
      "source": [
        "def rmse(y_pred, y):\n",
        "  \"\"\"loss function, rmse used\"\"\"\n",
        "  return ((y_pred - y)**2).mean()"
      ],
      "execution_count": 122,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I_F8e3W1RMWF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6b94121b-4ff1-433d-9540-423346e4f9ea"
      },
      "source": [
        "#Test the rmse with initial parameters\n",
        "y_pred = lin_pred(x, 0, 5)\n",
        "rmse(y_pred, y)"
      ],
      "execution_count": 123,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "107.38636363636364"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 123
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CznpK6MgUVRc"
      },
      "source": [
        "For gradient descent, need to calculate the derivative of the loss function (RMSE) with respect to w and b to create a gradient vector. The gradient values are multiplied by the learning rate and then subtracted from the current w and b for the next iteration. For the next iteration, RMSE is calculated again, derivatives calculated and a new gradient vector used to update w and b and the process repeats.\n",
        "\n",
        "Chain rule states:\n",
        "\n",
        "$\\dfrac{\\mathrm{d}f}{\\mathrm{d}w} = \\dfrac{\\mathrm{d}f}{\\mathrm{d}x} \\dfrac{\\mathrm{d}x}{\\mathrm{d}w}$ and $\\dfrac{\\mathrm{d}f}{\\mathrm{d}b} = \\dfrac{\\mathrm{d}f}{\\mathrm{d}x} \\dfrac{\\mathrm{d}x}{\\mathrm{d}b}$\n",
        "\n",
        "This means to calculate the gradient vector for w and b with respect to the RMSE function, we calculate the derivative of the RMSE function with respect to the output of the model (recalling derivative of x^2 = 2x) multiplied by the derivative of the model with respect to the parameter (weight or bias)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8NNcXAWWUTl1"
      },
      "source": [
        "def d_rmse(y_pred, y):\n",
        "  \"\"\"the derivative of rmse function\n",
        "  the derivative of f(x) = x**2 is f'(x)= 2*x\n",
        "  the rmse function is ((y_pred - y)**2) therefore:\"\"\"\n",
        "  return 2*(y-y_pred)"
      ],
      "execution_count": 124,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VUw928PiQRmj"
      },
      "source": [
        "def d_w(y, x, w, b):\n",
        "  \"\"\"here need to take the derivative of the inner function\n",
        "  y - (x * w + b) with respect to w\n",
        "  y - (x * w + b) = y - x * w - b = 0 - x - 0 = -x \"\"\"\n",
        "  return -x"
      ],
      "execution_count": 125,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FzwerUntStxR"
      },
      "source": [
        "def d_b(y, x, w, b):\n",
        "  \"\"\"here need to take the derivative of the inner function\n",
        "  y - (x * w + b) with respect to b\n",
        "  y - (x * w + b) = y - x * w - b = 0 - 0 - 1 = -1 \"\"\"\n",
        "  return -1"
      ],
      "execution_count": 126,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1aWYAWvwar0b"
      },
      "source": [
        "def grad_fn(x, y, y_pred, w, b):\n",
        "  \"\"\"want to return the gradient vector for w and b\n",
        "  with respect to the loss function rmse\"\"\"\n",
        "  dloss_ypred = d_rmse(y_pred, y)\n",
        "  dloss_w = dloss_ypred * d_w(y, x, w, b) #applying the chain rule\n",
        "  dloss_b = dloss_ypred * d_b(y, x, w, b) #applying the chain rule\n",
        "  return np.array([dloss_w.mean(), dloss_b.mean()]) \n"
      ],
      "execution_count": 127,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_fyXKOspaFRA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "27d6ea77-95e6-41fc-a674-322756db7328"
      },
      "source": [
        "w = 0\n",
        "b = 0\n",
        "y_pred = lin_pred(x, w, b)\n",
        "grad_fn(x, y, y_pred, w, b)"
      ],
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-1364.3,   -21. ])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 128
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fgJNBBDxNpt8"
      },
      "source": [
        "Above result shows that the two grad values for w and b are different orders of magnitude and so the learning rate to update one sensibly will be too small for the other. One way to solve this is to scale the values so in -1 to 1 range (min max scaling). However in this case we know works well if simple multiply x by 0.1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "24TL8mOSlv7H",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "02533b81-2446-4154-80cc-a240baccc180"
      },
      "source": [
        "x = x*0.1\n",
        "y_pred = lin_pred(x, w, b)\n",
        "grad_fn(x, y, y_pred, w, b)"
      ],
      "execution_count": 129,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-136.43,  -21.  ])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 129
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lA8H034lcHwi"
      },
      "source": [
        "def training_loop(n_epochs, learning_rate, params, x, y):\n",
        "  \"\"\"iterate over n epochs. For each iteration a new grad\n",
        "  of w,b with respect to loss function is calculated and\n",
        "  learning rate * grad is used to update w and b.\n",
        "  The loss after every 100 iterations is stored in a list and\n",
        "  returned for plotting\"\"\"\n",
        "  loss_vals =[]\n",
        "  for epoch in range(0, n_epochs+1):\n",
        "    w, b = params\n",
        "    y_pred = lin_pred(x, w, b)\n",
        "    loss = rmse(y_pred, y)\n",
        "    grad = grad_fn(x, y, y_pred, w, b)\n",
        "    params -= learning_rate * grad\n",
        "    #print every 100 iterations\n",
        "    if epoch % 100 ==0:\n",
        "        loss_vals.append(loss)\n",
        "        print(f\"epoch: {epoch} weight: {params[0]} bias: {params[1]} loss: {loss}\")\n",
        "  return loss_vals, w, b"
      ],
      "execution_count": 130,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pyBEgBEiel_j",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bbe3d999-06b1-46e5-b423-6df30d194df8"
      },
      "source": [
        "training_loss, w, b = training_loop(5000, 0.01, [0, 0], x, y)"
      ],
      "execution_count": 131,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch: 0 weight: 1.3643 bias: 0.21 loss: 187.38636363636363\n",
            "epoch: 100 weight: 2.7342259414252648 bias: -2.397056118790444 loss: 22.459697581227907\n",
            "epoch: 200 weight: 3.1459794955058857 bias: -4.727918563979814 loss: 16.829406108479255\n",
            "epoch: 300 weight: 3.4933543054368017 bias: -6.694344574996666 loss: 12.82209731319516\n",
            "epoch: 400 weight: 3.786416156096415 bias: -8.353314740646871 loss: 9.96993169113339\n",
            "epoch: 500 weight: 4.033657020594096 bias: -9.75290058577831 loss: 7.939928726239765\n",
            "epoch: 600 weight: 4.242241129852861 bias: -10.933657526711324 loss: 6.495092561887586\n",
            "epoch: 700 weight: 4.418212568569782 bias: -11.929800034447648 loss: 5.466743550619355\n",
            "epoch: 800 weight: 4.566670413005514 bias: -12.770193058040736 loss: 4.7348254860888375\n",
            "epoch: 900 weight: 4.69191648552276 bias: -13.479188436331395 loss: 4.213889454336911\n",
            "epoch: 1000 weight: 4.797580007316559 bias: -14.077330534522954 loss: 3.8431179672968594\n",
            "epoch: 1100 weight: 4.886722761364447 bias: -14.581951552633925 loss: 3.579224730463991\n",
            "epoch: 1200 weight: 4.961927812874555 bias: -15.007673755918471 loss: 3.3914011170516436\n",
            "epoch: 1300 weight: 5.025374358059818 bias: -15.366833180248253 loss: 3.2577193668914077\n",
            "epoch: 1400 weight: 5.0789008701107345 bias: -15.669837090050647 loss: 3.162572595933101\n",
            "epoch: 1500 weight: 5.124058372130283 bias: -15.925465546764148 loss: 3.094852746268209\n",
            "epoch: 1600 weight: 5.162155380706252 bias: -16.14112582627784 loss: 3.0466537586152667\n",
            "epoch: 1700 weight: 5.194295822438595 bias: -16.323067057539316 loss: 3.012348567120252\n",
            "epoch: 1800 weight: 5.221411022118652 bias: -16.47656130185537 loss: 2.9879321587926766\n",
            "epoch: 1900 weight: 5.244286689472948 bias: -16.606056319970826 loss: 2.9705540020286945\n",
            "epoch: 2000 weight: 5.263585686459107 bias: -16.71530445361548 loss: 2.9581852568610247\n",
            "epoch: 2100 weight: 5.279867234835524 bias: -16.807471356084264 loss: 2.9493819148783005\n",
            "epoch: 2200 weight: 5.293603120577342 bias: -16.885227722507146 loss: 2.943116216214911\n",
            "epoch: 2300 weight: 5.30519136468965 bias: -16.950826677851623 loss: 2.938656662125409\n",
            "epoch: 2400 weight: 5.314967756553306 bias: -17.00616906510881 loss: 2.9354826150245774\n",
            "epoch: 2500 weight: 5.3232155840020114 bias: -17.05285852550099 loss: 2.9332235158419935\n",
            "epoch: 2600 weight: 5.330173842076393 bias: -17.09224796675482 loss: 2.9316156225264165\n",
            "epoch: 2700 weight: 5.336044158317954 bias: -17.125478765938542 loss: 2.9304712190694038\n",
            "epoch: 2800 weight: 5.340996635275193 bias: -17.153513842833192 loss: 2.9296567003076603\n",
            "epoch: 2900 weight: 5.345174779518658 bias: -17.177165562195817 loss: 2.929076973980232\n",
            "epoch: 3000 weight: 5.348699659991585 bias: -17.197119273430854 loss: 2.928664359034448\n",
            "epoch: 3100 weight: 5.351673416191568 bias: -17.213953169772484 loss: 2.928370684096076\n",
            "epoch: 3200 weight: 5.354182217838848 bias: -17.2281550424318 loss: 2.9281616636272036\n",
            "epoch: 3300 weight: 5.356298761792726 bias: -17.240136415189347 loss: 2.92801289520778\n",
            "epoch: 3400 weight: 5.358084378568593 bias: -17.250244469007274 loss: 2.92790701063706\n",
            "epoch: 3500 weight: 5.359590809495489 bias: -17.258772102197025 loss: 2.927831648255982\n",
            "epoch: 3600 weight: 5.360861706010453 bias: -17.265966417653665 loss: 2.9277780097660466\n",
            "epoch: 3700 weight: 5.361933894534269 bias: -17.2720358830889 loss: 2.9277398330582263\n",
            "epoch: 3800 weight: 5.3628384435806105 bias: -17.277156371742986 loss: 2.9277126611333313\n",
            "epoch: 3900 weight: 5.363601564019846 bias: -17.28147625861547 loss: 2.9276933217626935\n",
            "epoch: 4000 weight: 5.364245368584266 bias: -17.28512071988696 loss: 2.9276795571413685\n",
            "epoch: 4100 weight: 5.36478851262269 bias: -17.28819536011519 loss: 2.927669760297213\n",
            "epoch: 4200 weight: 5.365246734671395 bias: -17.29078927230927 loss: 2.927662787482411\n",
            "epoch: 4300 weight: 5.365633312505386 bias: -17.292977619553536 loss: 2.9276578246448306\n",
            "epoch: 4400 weight: 5.365959447884853 bias: -17.294823812987723 loss: 2.927654292390282\n",
            "epoch: 4500 weight: 5.366234591145502 bias: -17.296381349254116 loss: 2.927651778340201\n",
            "epoch: 4600 weight: 5.366466715038362 bias: -17.297695360655194 loss: 2.9276499889879\n",
            "epoch: 4700 weight: 5.366662545753997 bias: -17.298803922940003 loss: 2.9276487154326603\n",
            "epoch: 4800 weight: 5.366827757825514 bias: -17.299739158614777 loss: 2.9276478089912916\n",
            "epoch: 4900 weight: 5.366967138557928 bias: -17.300528167747903 loss: 2.9276471638398816\n",
            "epoch: 5000 weight: 5.367084726748605 bias: -17.301193813241255 loss: 2.9276467046592276\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w0bh32ublZt6"
      },
      "source": [
        "The final parameters have settled at 5.37 and -17.3. As x was multiplied by 0.1, w should actually be 0.537.\n",
        "How well does this calculate 32F = 0C:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gCC6MTUxnVTo",
        "outputId": "7f4af70b-214e-46ec-c913-70948abb9708"
      },
      "source": [
        "32*(w/10) + b"
      ],
      "execution_count": 134,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-0.12652003077642604"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 134
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TvHm9OSElJFD"
      },
      "source": [
        "Plot the loss over the iterations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "id": "8P4viNFWgotm",
        "outputId": "badb8ade-866f-413d-a452-f3a5d214ef4a"
      },
      "source": [
        "plt.plot([i for i in range(0, 5100, 100)], training_loss)"
      ],
      "execution_count": 135,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f9f1c26cc18>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 135
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}